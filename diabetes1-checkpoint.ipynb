{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9839470-3118-4ff0-bb56-1a0831cfd293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            1       85             66             29        0  26.6   \n",
      "2            8      183             64              0        0  23.3   \n",
      "3            1       89             66             23       94  28.1   \n",
      "4            0      137             40             35      168  43.1   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.627   50        1  \n",
      "1                     0.351   31        0  \n",
      "2                     0.672   32        1  \n",
      "3                     0.167   21        0  \n",
      "4                     2.288   33        1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\POOJITHA\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\core.py:158: UserWarning: [03:37:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6688311688311688\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.70      0.73        99\n",
      "           1       0.53      0.62      0.57        55\n",
      "\n",
      "    accuracy                           0.67       154\n",
      "   macro avg       0.65      0.66      0.65       154\n",
      "weighted avg       0.68      0.67      0.67       154\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[69 30]\n",
      " [21 34]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load your uploaded dataset\n",
    "file_path = \"diabetes.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Inspect the first few rows to ensure the data is loaded correctly\n",
    "print(data.head())\n",
    "\n",
    "# Splitting features and target variable\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Standardize the feature data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define base learners\n",
    "base_learners = {\n",
    "    \"random_forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"svm\": SVC(kernel='linear', probability=True, random_state=42),\n",
    "    \"knn\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"logistic_regression\": LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Train base learners and get their predictions on the training data for stacking\n",
    "base_predictions_train = np.zeros((X_train.shape[0], len(base_learners)))\n",
    "base_predictions_test = np.zeros((X_test.shape[0], len(base_learners)))\n",
    "\n",
    "for i, (name, model) in enumerate(base_learners.items()):\n",
    "    # Train each base learner\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Cross-validated predictions on training data (for meta-learner input)\n",
    "    base_predictions_train[:, i] = cross_val_predict(\n",
    "        model, X_train, y_train, cv=5, method='predict_proba'\n",
    "    )[:, 1]\n",
    "\n",
    "    # Predictions on the test data\n",
    "    base_predictions_test[:, i] = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Define the meta-learners (XGBoost and Gradient Boosting)\n",
    "meta_model_xgb = xgb.XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "meta_model_gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train meta-learners using the predictions from base learners\n",
    "meta_model_xgb.fit(base_predictions_train, y_train)\n",
    "meta_model_gb.fit(base_predictions_train, y_train)\n",
    "\n",
    "# Predict on the test data using both meta-learners\n",
    "xgb_preds = meta_model_xgb.predict(base_predictions_test)\n",
    "gb_preds = meta_model_gb.predict(base_predictions_test)\n",
    "\n",
    "# Combine predictions from both meta-learners (Majority Vote)\n",
    "final_predictions = (xgb_preds + gb_preds) / 2\n",
    "final_predictions = np.where(final_predictions >= 0.5, 1, 0)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, final_predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, final_predictions))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, final_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bff3690-abad-4df6-bb92-fc76a35ee49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            1       85             66             29        0  26.6   \n",
      "2            8      183             64              0        0  23.3   \n",
      "3            1       89             66             23       94  28.1   \n",
      "4            0      137             40             35      168  43.1   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.627   50        1  \n",
      "1                     0.351   31        0  \n",
      "2                     0.672   32        1  \n",
      "3                     0.167   21        0  \n",
      "4                     2.288   33        1  \n",
      "Accuracy: 0.8\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79        99\n",
      "           1       0.77      0.85      0.81       101\n",
      "\n",
      "    accuracy                           0.80       200\n",
      "   macro avg       0.80      0.80      0.80       200\n",
      "weighted avg       0.80      0.80      0.80       200\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[74 25]\n",
      " [15 86]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"diabetes.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Inspect the data (optional)\n",
    "print(data.head())\n",
    "\n",
    "# Split features and target\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Handle imbalanced data with SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define base learners with optimized hyperparameters\n",
    "base_learners = {\n",
    "    \"random_forest\": RandomForestClassifier(n_estimators=200, max_depth=8, random_state=42),\n",
    "    \"svm\": SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42),\n",
    "    \"knn\": KNeighborsClassifier(n_neighbors=7),\n",
    "    \"logistic_regression\": LogisticRegression(C=1.0, max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Train base learners and collect predictions\n",
    "base_predictions_train = np.zeros((X_train.shape[0], len(base_learners)))\n",
    "base_predictions_test = np.zeros((X_test.shape[0], len(base_learners)))\n",
    "\n",
    "for i, (name, model) in enumerate(base_learners.items()):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Cross-validated predictions on training data (for meta-learner)\n",
    "    base_predictions_train[:, i] = cross_val_predict(\n",
    "        model, X_train, y_train, cv=5, method='predict_proba'\n",
    "    )[:, 1]\n",
    "    \n",
    "    # Predictions on test data\n",
    "    base_predictions_test[:, i] = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Meta-learners with optimized parameters\n",
    "meta_model_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=200, learning_rate=0.1, max_depth=4, random_state=42, eval_metric='logloss'\n",
    ")\n",
    "meta_model_gb = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Train the meta-learners on the base learners' predictions\n",
    "meta_model_xgb.fit(base_predictions_train, y_train)\n",
    "meta_model_gb.fit(base_predictions_train, y_train)\n",
    "\n",
    "# Get predictions from both meta-learners\n",
    "xgb_preds = meta_model_xgb.predict(base_predictions_test)\n",
    "gb_preds = meta_model_gb.predict(base_predictions_test)\n",
    "\n",
    "# Combine predictions using majority voting\n",
    "final_predictions = (xgb_preds + gb_preds) / 2\n",
    "final_predictions = np.where(final_predictions >= 0.5, 1, 0)\n",
    "\n",
    "# Evaluate the optimized model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, final_predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, final_predictions))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, final_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f850e6ac-6fc4-430a-8618-18e715323815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
